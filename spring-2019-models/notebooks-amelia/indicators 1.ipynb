{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandana as pdna\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare(table, column):\n",
    "#     return len(table[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedata_file_path = '/home/data/'\n",
    "dd = '/home/data/spring_2019/base/'\n",
    "shapefiles_path = '/home/simon/spatial-data/'\n",
    "model_outputs_path = '/home/data/spring_2019/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''gets all relevant tables from the HDF5 and saves them to a dictionary that is indexed by 1) year and \n",
    "    2) table name. Closes the store. The function is assigned to the global variable data_dict'''\n",
    "    data_dict = {}\n",
    "    hdfstore = pd.HDFStore(model_outputs_path+'model_data_output.h5', mode='r') #sometimes doesnt work with a relative file path\n",
    "    for year in [2010, 2015, 2025]:\n",
    "        persons = hdfstore[\"/{}/persons\".format(year)]\n",
    "        hh = hdfstore[\"/{}/households\".format(year)]\n",
    "        buildings = hdfstore[\"/{}/buildings\".format(year)]\n",
    "        parcels = hdfstore[\"/{}/parcels\".format(year)]\n",
    "        jobs = hdfstore[\"/{}/jobs\".format(year)]\n",
    "        data_dict[year] = {\"hh\": hh, \"buildings\": buildings, \"parcels\": parcels, \"jobs\": jobs, \"persons\": persons}\n",
    "#         data_dict.update({year: {\"hh\": hh, \"buildings\": buildings, \"parcels\": parcels, \"jobs\": jobs}})\n",
    "    print(\"closing store\")\n",
    "    hdfstore.close()\n",
    "    return data_dict\n",
    "\n",
    "data_dict = get_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[2010]['buildings'].reset_index()[[\"building_id\", \"parcel_id\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_data(year):\n",
    "    '''merges the household and buildings and parcels and persons tables together in an inner merge, \n",
    "    resulting in a dataframe with every person, their household information and their geographic information\n",
    "    like block group id and parcel id, and node id'''\n",
    "        \n",
    "    print(\"merging buildings to parcels\")\n",
    "    try:\n",
    "        buildings = data_dict[year][\"buildings\"].reset_index()[[\"building_id\", \"parcel_id\"]]\n",
    "        parcels = data_dict[year][\"parcels\"].reset_index()[[\"parcel_id\", \"county_id\", \"node_id\"]]\n",
    "        bldg_parc = pd.merge(buildings, parcels, on=\"parcel_id\")\n",
    "        hh = data_dict[year][\"hh\"]\n",
    "        persons = data_dict[year][\"persons\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        import pdb; pdb.set_trace()\n",
    "    \n",
    "    print(\"merging households to buildings and parcels table\")\n",
    "    \n",
    "    hh_build = pd.merge(hh, bldg_parc, on=\"building_id\")\n",
    "    \n",
    "    print(\"merging households to buildings\")\n",
    "\n",
    "    hh_build.index.name = \"household_id\"\n",
    "    \n",
    "    hh_build = hh_build.reset_index()\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"merging persons to households and buildings\")\n",
    "    persons = data_dict[year][\"persons\"]\n",
    "    persons.index.name = \"person_id\"\n",
    "    \n",
    "    buildings_persons = pd.merge(persons, hh_build, on='household_id')\n",
    "    print(\"hh build table has \", len(hh_build), \"rows\")\n",
    "    \n",
    "    buildings_persons[\"block_group_id\"] = buildings_persons[\"block_group_id\"].apply(lambda x: \"0\"+x)\n",
    "    buildings_persons[\"census_tract\"] = buildings_persons[\"block_group_id\"].apply(lambda x: x[:-1])\n",
    "    return buildings_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_locations(year):\n",
    "    '''get the location of the jobs by merging the jobs table with the buildings table on building_id and that\n",
    "    table with the parcels table on parcel_id, resulting in each job being assigned to a node id to be used\n",
    "    in the pandana network'''\n",
    "    \n",
    "    buildings = data_dict[year]['buildings'].reset_index()\n",
    "    jobs = data_dict[year]['jobs'].reset_index()\n",
    "    parcels = data_dict[year]['parcels'].reset_index()\n",
    "    \n",
    "    print(\"merging buildings to jobs\")\n",
    "    jobs_buildings = pd.merge(buildings, jobs, on='building_id')\n",
    "    \n",
    "    print(\"merging buildings with jobs to parcels\")\n",
    "    jobs_merge = pd.merge(jobs_buildings, parcels, on='parcel_id')\n",
    "    return jobs_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workers_by_race(year):\n",
    "    '''Get the number of workers for each race by node, for a given year. Returns a dataframe\n",
    "    with a column for each race and the number of workers assigned to that node by race. The node assignments\n",
    "    are obtained from the person-> parcel match in get_merged_data()'''\n",
    "    \n",
    "    node_df_lol = []\n",
    "    node_id_list = []\n",
    "    df = get_merged_data(year)\n",
    "    for node_id, node_df in df.groupby(\"node_id\"):\n",
    "        nhwhite_workers = sum((node_df[\"race_id\"] == 1) & (node_df[\"hispanic\"] == 1) & (node_df[\"worker\"] == 1))\n",
    "        nhblack_workers = sum((node_df[\"race_id\"] == 2) & (node_df[\"hispanic\"] == 1) & (node_df[\"worker\"] == 1))\n",
    "        nhasian_workers = sum((node_df[\"race_id\"] == 6) & (node_df[\"hispanic\"] == 1) & (node_df[\"worker\"] == 1))\n",
    "        hispanic_workers = sum((node_df[\"hispanic\"] != 1) & (node_df[\"worker\"] == 1))\n",
    "        non_workers_any = sum(node_df[\"worker\"] == 0)\n",
    "        block = node_df['block_group_id'].iloc[0] # i think this is ok for now because they should all be the same\n",
    "        node_id_list.append(node_id)\n",
    "\n",
    "        row = [nhwhite_workers, nhblack_workers, nhasian_workers, hispanic_workers, non_workers_any, block]\n",
    "        node_df_lol.append(row)\n",
    "        \n",
    "    workers_by_node = pd.DataFrame(node_df_lol, columns=[\"nhwhite_workers\", \"nhblack_workers\",\"nhasian_workers\",\n",
    "            \"hispanic_workers\", \"non_workers_any\", \"block_group\"], index=node_id_list)\n",
    "    \n",
    "    \n",
    "    return workers_by_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandana Accessibility Calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_network_links = pd.read_csv('beam-network-links.csv')\n",
    "beam_network_nodes = pd.read_csv('beam-network-nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=pdna.Network(beam_network_nodes.x, beam_network_nodes.y, beam_network_links[\"from\"], beam_network_links[\"to\"],\n",
    "                 beam_network_links[[\"travelTime\"]])\n",
    "net.precompute(10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_accessibilities(net, year, dist):\n",
    "    '''returns the number of jobs within a radius of dist of each node'''\n",
    "    df = get_job_locations(year)\n",
    "    workers_df = get_workers_by_race(year)\n",
    "    node_ids = df[\"node_id\"]\n",
    "    n = net.set(node_ids)\n",
    "    s = net.aggregate(dist, type=\"sum\", decay=\"linear\")\n",
    "    access = pd.DataFrame(s)\n",
    "    access = access.rename({0:\"jobs\"}, inplace=False, axis=1)\n",
    "    access = pd.merge(workers_df, access, right_index=True, left_index=True)\n",
    "    return access\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an intermediate step to save data just in case\n",
    "get_job_accessibilities(net, 2025, 10000).to_csv('indicators_output/accessibilities_2025_1000.csv', index=False)\n",
    "get_job_accessibilities(net, 2015, 10000).to_csv('indicators_output/accessibilities_2015_1000.csv', index=False)\n",
    "get_job_accessibilities(net, 2010, 10000).to_csv('indicators_output/accessibilities_2010_1000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{weighted accessibility} = E[\\text{# of accessible jobs}|\\text{race R}] =$$\n",
    "\n",
    "$$\\sum_{i=1}^n P(\\text{person of race R and person assigned to node}_i) \\times \\text{# jobs assigned to node}_i =$$\n",
    "\n",
    "$$\\sum_{i=1}^n P(\\text{person of race R | person assigned to node}_i) \\times P(\\text{person assigned to node}_i) \\times \\text{# jobs assigned to node}_i =$$\n",
    "\n",
    "$$\\sum_{i=1}^n \\frac{\\text{# persons of race R assigned to node}_i}{\\text{# persons assigned to node}_i} \\times \\frac{\\text{# persons assigned to node}_i}{\\text{# of people in blk group}} \\times (\\text{# of jobs assigned to node}_i) =$$\n",
    "\n",
    "$$\\sum_{i=1}^n \\frac{\\text{# persons of race R assigned to node}_i}{\\text{# of people in blk group}} \\times (\\text{# of jobs assigned to node}_i)$$\n",
    "\n",
    "where $i$ is a node in the block group/other geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accessibilities_by_race(year, dist, net, geography): \n",
    "    '''This is the the essence of the computation being performed to get the race-based accessibilities. This function\n",
    "    takes in the year, buffer distance, pandana network (should be precomputed) and desired geography (right now this\n",
    "    notebook has shapefiles set up for census tract (\"census_tract\") and block group (\"block_group\"). Returns a table indexed by \n",
    "    the id of the geography, and a column for each weighted accessibility by race.'''\n",
    "    geo_df_lol = []\n",
    "    geo_id_list = []\n",
    "    \n",
    "    df = get_job_accessibilities(net, year, dist)\n",
    "    for geo, geo_df in df.groupby(geography):\n",
    "        people_per_geo = sum(geo_df[\"nhwhite_workers\"] + geo_df[\"nhblack_workers\"]+geo_df[\"nhasian_workers\"]+geo_df[\"hispanic_workers\"])+1e-6\n",
    "        node_access_nhw = sum(geo_df[\"nhwhite_workers\"]*geo_df[\"jobs\"])/people_per_geo\n",
    "        node_access_nhb = sum(geo_df[\"nhblack_workers\"]*geo_df[\"jobs\"])/people_per_geo\n",
    "        node_access_nha = sum(geo_df[\"nhasian_workers\"]*geo_df[\"jobs\"])/people_per_geo\n",
    "        node_access_h = sum(geo_df[\"hispanic_workers\"]*geo_df[\"jobs\"])/people_per_geo\n",
    "        geo_id_list.append(geo)\n",
    "        row = [node_access_nhw, node_access_nhb, node_access_nha, node_access_h]\n",
    "        geo_df_lol.append(row)\n",
    "        \n",
    "    access_geo = pd.DataFrame(geo_df_lol, columns=[\"access_nhwhite\", \"access_nhblack\",\"access_nhasian\",\n",
    "              \"access_hispanic\"], index=geo_id_list)\n",
    "    \n",
    "    \n",
    "    return access_geo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_county_codes = ['001', '013', '097', '095', '081', '085', '075', '041', '055'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_groups = gp.read_file(shapefiles_path+'tl_2018_06_bg/tl_2018_06_bg.shp')\n",
    "bay_bg = block_groups[(block_groups[\"COUNTYFP\"].isin(bay_county_codes)) & (block_groups[\"ALAND\"]> 100)]\n",
    "\n",
    "tracts = gp.read_file(shapefiles_path+'tl_2018_06_tract/tl_2018_06_tract.shp')\n",
    "bay_tracts = tracts[(tracts[\"COUNTYFP\"].isin(bay_county_codes)) & (tracts[\"ALAND\"]> 100)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geographies(year, dist, net, geography):\n",
    "    'merge the relevant shapefiles with the race weighted accessibilities tables to allow mapping'\n",
    "    df = accessibilities_by_race(year, dist, net, geography)\n",
    "    if (geography == 'census_tract'):\n",
    "        return pd.merge(bay_tracts, df, right_index=True, left_on='GEOID')\n",
    "    elif (geography == 'block_group'):\n",
    "        return pd.merge(bay_bg, df, right_index=True, left_on='GEOID')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##add the final access tables\n",
    "access_10000_2025_bg = merge_geographies(2025, 10000, net, \"block_group\").to_csv(\"indicators_output/access_10000_2025_bg.csv\")\n",
    "access_10000_2015_bg = merge_geographies(2015, 10000, net, \"block_group\").to_csv(\"indicators_output/access_10000_2015_bg.csv\")\n",
    "access_10000_2010_bg = merge_geographies(2010, 10000, net, \"block_group\").to_csv(\"indicators_output/access_10000_2010_bg.csv\")\n",
    "access_10000_2025_tract = merge_geographies(2025, 10000, net, \"census_tract\").to_csv(\"indicators_output/access_10000_2025_tract.csv\")\n",
    "access_10000_2015_tract = merge_geographies(2015, 10000, net, \"census_tract\").to_csv(\"indicators_output/access_10000_2015_tract.csv\")\n",
    "access_10000_2010_tract = merge_geographies(2010, 10000, net, \"census_tract\").to_csv(\"indicators_output/access_10000_2010_tract.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, hisp_25 = plt.subplots(1, figsize=(10,8))\n",
    "# hisp_25 = block_access_shp.plot(ax=hisp_25, column='access_hispanic', legend = True, cmap='OrRd')\n",
    "# hisp_25.set_title(\"hispanic job access 2025\")\n",
    "# plt.savefig(\"bg_his_jobs_10000m_2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, nhw_25 = plt.subplots(1, figsize=(10,8))\n",
    "# nhw_25 = block_access_shp.plot(axes=nhw_25, column='access_nhwhite', legend = True, cmap='PuBu')\n",
    "# nhw_25.set_title(\"Non hispanic white job access 2025\")\n",
    "# plt.savefig(\"bg_nhw_jobs_10000m_2025\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, nha_25 = plt.subplots(1, figsize=(10,8))\n",
    "# nha_25 = block_access_shp.plot(axes=nha_25, column='access_nhasian', legend = True, cmap='YlGn')\n",
    "# nha_25.set_title(\"Non hispanic asian job access 2025\")\n",
    "# plt.savefig(\"bg_nha_jobs_10000m_2025\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, nhb_25 = plt.subplots(1, figsize=(10,8))\n",
    "# nhb_25 = block_access_shp.plot(ax = nhb_25, column='access_nhblack', legend = True, cmap='Purples')\n",
    "# nhb_25.set_title(\"Non hispanic black job access 2025\")\n",
    "# plt.savefig(\"bg_nhb_jobs_10000m_2025\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
